{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba430f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_centered' from 'scipy.signal.signaltools' (/opt/anaconda3/lib/python3.8/site-packages/scipy/signal/signaltools.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c1dce90875ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# imbelearn package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/statsmodels/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                    \u001b[0mZeroInflatedGeneralizedPoisson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                    ZeroInflatedNegativeBinomialP)\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurvfunc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSurvfuncRight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazard_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPHReg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvector_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvecm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVECM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvector_ar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvar_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtsatools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtsatools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madd_trend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetrend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlagmat2ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_lag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/filters/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhp_filter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhpfilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcf_filter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcffilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfiltertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmiso_lfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvolution_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/filters/filtertools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfftpack\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignaltools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_centered\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrim_centered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPandasWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_centered' from 'scipy.signal.signaltools' (/opt/anaconda3/lib/python3.8/site-packages/scipy/signal/signaltools.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "import missingno as msno\n",
    "import plotly.graph_objects as go\n",
    "from datetime import date\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# topic modeling packages \n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import re\n",
    "from pprint import pprint# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel# spaCy for preprocessing\n",
    "from gensim import similarities\n",
    "import spacy# Plotting tools\n",
    "import pyLDAvis\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "\n",
    "# sklearn packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix,classification_report, precision_recall_curve, f1_score, auc\n",
    "from xgboost import XGBClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# imbelearn package\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b843f023",
   "metadata": {},
   "source": [
    "## 1. Tech Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426b805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tech = pd.DataFrame(pd.read_csv(\"Tech_cleaned.csv\"))\n",
    "print(\"Technology Shape:\", df_tech.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e1e703",
   "metadata": {},
   "source": [
    "### 1.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only patent IP_Type\n",
    "df_tech = df_tech.loc[df_tech['IP_Type']=='Patent'].reset_index()\n",
    "df_tech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping major divisions with sufficient number of patents; in case of multiple divisions, search through the\n",
    "# list of divisions in the order of the following dict\n",
    "division_mapping = {'PSD':'PSD', \n",
    "                    'BSD':'BSD', \n",
    "                    'PME':'PME', \n",
    "                    'Argonne National Laboratory':'ANL', \n",
    "                    'Marine Biological Laboratory':'MBL', \n",
    "                    'Booth':'Booth', \n",
    "                    'University of Chicago Hospital':'UCH', \n",
    "                    'SSD':'SSD', \n",
    "                    'Comprehensive Cancer Center':'CCC', \n",
    "                    'University of Chicago':'UC',\n",
    "                    'Toyota Technological Institute':'TTI', \n",
    "                    'Humanities':'Humanities', \n",
    "                    'Harris':'Harris',\n",
    "                    'Institute of Politics':'Politics'}\n",
    "\n",
    "df_tech.loc[df_tech['Division_Department'].isnull()] = 'NA'\n",
    "\n",
    "df_tech['Primary_Division'] = 'Others'\n",
    "df_tech.loc[df_tech['Division_Department']=='NA', 'Primary_Division'] = 'NA'\n",
    "\n",
    "for i in range(len(df_tech)):\n",
    "    for key in division_mapping:\n",
    "        if key in df_tech['Division_Department'][i]:\n",
    "            df_tech['Primary_Division'][i] = division_mapping.get(key)\n",
    "            break;\n",
    "            \n",
    "df_tech[['Division_Department','Primary_Division']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b60f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tech['Primary_Division'].value_counts()/len(df_tech)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any Divisions with less than 0.5% of total records will be consolidated into 'Other' category \n",
    "other = ('Humanities', 'SSD', 'UCH', 'Booth', 'UC', 'Harris', 'TTI', 'CCC', 'Others')\n",
    "\n",
    "df_tech.loc[df_tech['Primary_Division'].isin(other), 'Primary_Division'] = 'Other'\n",
    "\n",
    "# Confrim changes, % breakout\n",
    "df_tech['Primary_Division'].value_counts()/len(df_tech)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c62ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all description columns are listed as null and not blank strings (this will impact column merging)\n",
    "df_tech['Brief_Technology_Description'] = df_tech.Brief_Technology_Description.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_tech['Assessment_Description'] = df_tech.Assessment_Description.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_tech['Abstract'] = df_tech.Abstract.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Some of the descriptions include \"See ...\". We want to remove these, as they do not provide any insight\n",
    "df_tech['Brief_Technology_Description'] = df_tech.Brief_Technology_Description.replace(r'^SEE .*', np.nan, regex=True)\n",
    "df_tech['Brief_Technology_Description'] = df_tech.Brief_Technology_Description.replace(r'^See .*', np.nan, regex=True)\n",
    "\n",
    "df_tech['Assessment_Description'] = df_tech.Assessment_Description.replace(r'^SEE .*', np.nan, regex=True)\n",
    "df_tech['Assessment_Description'] = df_tech.Assessment_Description.replace(r'^See .*', np.nan, regex=True)\n",
    "\n",
    "df_tech['Abstract'] = df_tech.Abstract.replace(r'^SEE .*', np.nan, regex=True)\n",
    "df_tech['Abstract'] = df_tech.Abstract.replace(r'^See .*', np.nan, regex=True)\n",
    "\n",
    "# Merge Abstract, Assessment_Description, and Brief_Technology_Description together to populate null values in each\n",
    "# Brief_Technology_Description has the most information, so we will use this as the base column \n",
    "df_tech.Brief_Technology_Description = df_tech.Brief_Technology_Description.fillna(df_tech.Assessment_Description)\n",
    "df_tech.Brief_Technology_Description = df_tech.Brief_Technology_Description.fillna(df_tech.Abstract)\n",
    "df_tech.Brief_Technology_Description = df_tech.Brief_Technology_Description.fillna(df_tech.Title)\n",
    "\n",
    "del df_tech['Assessment_Description']\n",
    "del df_tech['Abstract']\n",
    "\n",
    "# Confrirm we are not seeing any common issues in Breif_Technology_Description field (NA will be removed later)\n",
    "df_tech.Brief_Technology_Description.value_counts().sort_values(ascending=False).nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3852e58",
   "metadata": {},
   "source": [
    "### 1.2 Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to patents that we are the leading institution\n",
    "df_tech = df_tech.loc[df_tech['We_are_not_the_lead_institution']=='No']\n",
    "\n",
    "# Generate lists for the will never be licensed, could be licensed, igone, and licensed statuses\n",
    "licensed = ('Non-Exclusively Licensed', 'Exclusively Licensed', 'Optioned','Seeking Licensees', 'Post Election Hold', 'IP Authorized', 'Pending Title Election Decision')\n",
    "never_licensed = ('Closed/Inactive', 'Waived Rights to Inventor', 'Awaiting Expiration','Licenses at Potential', 'IIA - Other Party Leads', 'Jointly Owned - UoC Leads', 'Combined with other Tech', 'Jointly Owned - Other Party Leads', 'Awaiting Info from Inventors', 'Negotiating License')\n",
    "\n",
    "df_tech.loc[df_tech['Status'].isin(licensed), 'License_Status'] = 'license'\n",
    "df_tech.loc[df_tech['Status'].isin(never_licensed), 'License_Status'] = 'no_license'\n",
    "\n",
    "df_tech.License_Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d294643",
   "metadata": {},
   "source": [
    "### 1.3 Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e33ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_keep = ['Tech ID',\n",
    "'Title',\n",
    "'Lead_Inventor',\n",
    "'Disclosure_Date',\n",
    "'Division_Department',\n",
    "'Owners',\n",
    "'Ability_of_investigator_to_continue_research',\n",
    "'Ability_to_advance_the_project_outside_the_lab',\n",
    "'Abstract',\n",
    "'Assessment_Description',\n",
    "'Brief_Technology_Description',\n",
    "'Compelling_nature_of_data',\n",
    "'Detectability_of_infringement_and_enforceability',\n",
    "'Development_and_regulatory_path_for_the_product',\n",
    "'Freedom-to-operate_FTO_issues',\n",
    "'Historical_cooperation_or_not_of_investigator',\n",
    "'Identity_of_the_eventual_product',\n",
    "'Impact_of_patent_on_adoption_of_technology',\n",
    "'Industrial_startup_co-ownership_of_the_IP',\n",
    "'Institution',\n",
    "'Licensing_interest_by_a_specific_company',\n",
    "'Market_feedback',\n",
    "'Market_Size',\n",
    "'Nature_of_improvement_over_existing_art',\n",
    "'Patentability_questions',\n",
    "'Risk_cost_sharing_w_other_institution',\n",
    "'Size_of_Market',\n",
    "'Stage_of_research',\n",
    "'License_Status']\n",
    "\n",
    "df_tech_keep = df_tech[['Tech_ID',\n",
    "'Title',\n",
    "'Lead_Inventor',\n",
    "'Disclosure_Date',\n",
    "'Division_Department',\n",
    "'Primary_Division',\n",
    "'Owners',\n",
    "'Ability_of_investigator_to_continue_research',\n",
    "'Ability_to_advance_the_project_outside_the_lab',\n",
    "'Brief_Technology_Description',\n",
    "'Compelling_nature_of_data',\n",
    "'Detectability_of_infringement_and_enforceability',\n",
    "'Development_and_regulatory_path_for_the_product',\n",
    "'Freedom-to-operate_FTO_issues',\n",
    "'Historical_cooperation_or_not_of_investigator',\n",
    "'Identity_of_the_eventual_product',\n",
    "'Impact_of_patent_on_adoption_of_technology',\n",
    "'Industrial_startup_co-ownership_of_the_IP',\n",
    "'Institution',\n",
    "'Licensing_interest_by_a_specific_company',\n",
    "'Market_feedback',\n",
    "'Market_Size',\n",
    "'Nature_of_improvement_over_existing_art',\n",
    "'Patentability_questions',\n",
    "'Risk_cost_sharing_w_other_institution',\n",
    "'Size_of_Market',\n",
    "'Stage_of_research',\n",
    "'License_Status']]\n",
    "\n",
    "df_tech_keep.rename(columns = {'Title':'Tech_Title'}, inplace = True)\n",
    "\n",
    "df_tech_keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85edaf8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tech_missing = df_tech_keep.isnull().sum()\n",
    "\n",
    "print(tech_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7efff9",
   "metadata": {},
   "source": [
    "## 2. Patent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat = pd.DataFrame(pd.read_csv(\"patentData_Cleaned.csv\"))\n",
    "print(\"Patent Shape:\", df_pat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6314016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927e757",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77755bd",
   "metadata": {},
   "source": [
    "### Consolidate the \"File_Date\" and \"Date_Actually_Filed\" into a new column named \"Actually_File_Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take \"Date_Actually_Filed\" as main column and combine \"File_Date\" which generate a new column named \"Actually_File_Date\"\n",
    "\n",
    "df_pat[\"Actually_File_Date\"] = df_pat[\"Date_Actually_Filed\"].combine_first(df_pat[\"File_Date\"])\n",
    "df_pat.drop([\"Date_Actually_Filed\", \"File_Date\"], 1, inplace=True)\n",
    "\n",
    "# The original columns \"File_Date\" has 36 missing values and \"Date_Actually_Filed\" has 1470 missing values\n",
    "miss_num = df_pat[\"Actually_File_Date\"].isnull().sum()\n",
    "print(\"Actually_File_Date is missing:\", miss_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e65480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 35 null records in \"Actually_File_Date\"\n",
    "df_pat = df_pat[~df_pat['Actually_File_Date'].isna()]\n",
    "\n",
    "# Convert \"object\" data type to \"datetime\"\n",
    "df_pat['Actually_File_Date'] = pd.to_datetime(df_pat['Actually_File_Date'].astype(str),format='%m/%d/%Y')\n",
    "\n",
    "# Confirm null records have been removed \n",
    "df_pat[\"Actually_File_Date\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1096df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split \"Inventors\" names and count the Number of Inventors for each patent\n",
    "\n",
    "# N is the number of inventors for each patent, the range of N is [1,19] and the average of N is 3. \n",
    "# For modeling purpose, we keep the first 5 inventors and split into multi-columns\n",
    "n = 5  \n",
    "inventor_names = [f'Inventors_{i}' for i in range(n)]\n",
    "df_new = df_pat['Inventors'].map(lambda x:(str(x).split(','),len(str(x).split(',')))).apply(pd.Series)\n",
    "df_inventor = df_new[0].apply(lambda x:x[:n]).apply(pd.Series)\n",
    "df_inventor.columns=inventor_names\n",
    "df_inventor.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for counting the total number of inventors for each patent\n",
    "df_cnt = pd.DataFrame(df_new[1])\n",
    "df_cnt.columns=['Number_of_Inventors']\n",
    "df_cnt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining 'Number_of_Inventors' to the patent dataset\n",
    "df_pat = pd.concat([df_pat, df_cnt],axis=1).drop(\"Inventors\", axis = 1)\n",
    "df_pat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the null records in \"Patent_Status\"\n",
    "df_pat = df_pat[~df_pat['Status'].isna()]\n",
    "\n",
    "# Confirm removal \n",
    "df_pat.Status.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final column list of Patent data before feature selection\n",
    "df_pat.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46551333",
   "metadata": {},
   "source": [
    "### 2.2 Drop Columns for Patent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b39741",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_keep_pat = ['Tech_ID',\n",
    "'Title',\n",
    "'Country_WIPO_ID',\n",
    "'Actually_File_Date',\n",
    "'Is_Priority',\n",
    "'Lawfirm',\n",
    "'Attorney',\n",
    "'Number_of_Inventors',\n",
    "'Application_Type'\n",
    "]\n",
    "\n",
    "df_pat_keep = df_pat[['Tech_ID',\n",
    "'Title',\n",
    "'Country_WIPO_ID',\n",
    "'Actually_File_Date',\n",
    "'Is_Priority',\n",
    "'Lawfirm',\n",
    "'Attorney',\n",
    "'Number_of_Inventors',\n",
    "'Application_Type']]\n",
    "\n",
    "df_pat_keep.rename(columns = {'Title':'Patent_Title'}, inplace = True)\n",
    "\n",
    "# Check missing values in columns we keep and impute any null values with \"Others\"\n",
    "df_pat_keep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute for columns with missing values \n",
    "df_pat_keep[\"Is_Priority\"].fillna(\"Other\", inplace = True)\n",
    "df_pat_keep[\"Lawfirm\"].fillna(\"Other\", inplace = True)\n",
    "df_pat_keep[\"Attorney\"].fillna(\"Other\", inplace = True)\n",
    "\n",
    "df_pat_keep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slightly higher than the count for patent modeling. This is because some records with a NA status in Patent_Status were removed\n",
    "# We will not automatically remove these for the purposes of license modeling \n",
    "df_pat_keep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637534a6",
   "metadata": {},
   "source": [
    "### 3. Merge Tech and Patent datasets with columns_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795f225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are 5859 rows are matched with 41 columns (\"Tech ID\" will be dropped later)\n",
    "df_modeling = df_pat_keep.join(\n",
    "df_tech_keep.set_index([\"Tech_ID\"]),\n",
    "on=[\"Tech_ID\"],\n",
    "how=\"inner\",\n",
    "lsuffix=\"_x\",\n",
    "rsuffix=\"_y\")\n",
    "\n",
    "df_modeling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ce02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a6d62",
   "metadata": {},
   "source": [
    "### 3.1 Duplicate Detection and consolidation to unique records only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique Technology titles in the Modeling Dataset:\", df_modeling['Tech_Title'].nunique())\n",
    "\n",
    "print(\"Unique Patent titles in the Merged Dataset:\", df_modeling['Patent_Title'].nunique())\n",
    "\n",
    "print(\"Shape of the Merged dataset:\", df_modeling.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see there are 4,768 records in the dataset that have the same Tech_Title/Patent_Title combination\n",
    "\n",
    "duplicates = df_modeling[df_modeling.duplicated(subset=['Tech_Title','Patent_Title'], keep=False)]\n",
    "duplicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of the 10 technologies with the most assocaited rows in the dataset\n",
    "df_modeling.Tech_Title.value_counts().sort_values(ascending=False).nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7aadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review of column with missing data\n",
    "df_modeling.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1666dd9d",
   "metadata": {},
   "source": [
    "### Consolidate \"Application Type\" to determine a unique patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2680dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort values by Disclosure Date and Actually Filed Date\n",
    "df_modeling = df_modeling.sort_values(by=['Disclosure_Date','Actually_File_Date'])\n",
    "df_modeling.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asjust Patent_Title column to lowercase all values and remove extra whitespace to avoid duplicates \n",
    "# lowercase \n",
    "df_modeling['Patent_Title'] = df_modeling['Patent_Title'].str.lower()\n",
    "# remove extra white space \n",
    "df_modeling['Patent_Title'] = df_modeling['Patent_Title'].str.strip()\n",
    "\n",
    "# Group by Tech_Title, Patent_Title, Disclosure_Date, and Country_WIPO_ID. \n",
    "# These columns indicate a unique record for the purposes of modeling  \n",
    "df_modeling = df_modeling.groupby(['Tech_Title','Disclosure_Date','Country_WIPO_ID'])\n",
    "df_modeling.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first available Actually_Filed_Date from that unique entry\n",
    "df_modeling = df_modeling.first().reset_index()\n",
    "df_modeling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop one test row \"TEST01\" and then drop the \"Tech_ID\" column\n",
    "df_modeling = df_modeling[df_modeling[\"Tech_ID\"].str.contains(\"TEST01\") == False]\n",
    "df_modeling.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_modeling = df_modeling.drop(columns=['Tech_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the types of Application_Type values to determine if it looks like we are keeping to correct applications\n",
    "df_modeling['Application_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1ea1a",
   "metadata": {},
   "source": [
    "### 3.2 Incorporate LDA Topic modeling to add to columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fcdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tech_Titles to list and tokenize\n",
    "data = df_modeling.Tech_Title.values.tolist()\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60675169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data_lemmatized)  \n",
    "# Create Corpus \n",
    "texts = data_lemmatized  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop LDA Topic Model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46215b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keyword of topics\n",
    "pprint(lda_model.print_topics())\n",
    "# This applies the lda model to our corpus of titles, which we can use to assign a majority topic for each Tech_Topic \n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Model Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tuples containing topic percentages from each Tech_Title from doc_lda\n",
    "topics = pd.DataFrame(doc_lda)\n",
    "\n",
    "# Extract the 2nd element (the percentages for each given topic) from each column containing list element\n",
    "topics['topic1'] = list(zip(*topics[0]))[0]\n",
    "topics['topic2'] = list(zip(*topics[0]))[1]\n",
    "topics['topic3'] = list(zip(*topics[0]))[2]\n",
    "#topics['topic4'] = list(zip(*topics[0]))[3]\n",
    "\n",
    "topics['topic1'] = list(zip(*topics['topic1']))[1]\n",
    "topics['topic2'] = list(zip(*topics['topic2']))[1]\n",
    "topics['topic3'] = list(zip(*topics['topic3']))[1]\n",
    "#topics['topic4'] = list(zip(*topics['topic4']))[1]\n",
    "\n",
    "# convert series objects to float for comparison \n",
    "topics['topic1'] = topics['topic1'].astype(str).astype(float)\n",
    "topics['topic2'] = topics['topic2'].astype(str).astype(float)\n",
    "topics['topic3'] = topics['topic3'].astype(str).astype(float)\n",
    "#topics['topic4'] = topics['topic4'].astype(str).astype(float)\n",
    "\n",
    "# create 'lda_topic' column with topic that carries majority weight for Tech_Title\n",
    "topics.loc[(topics['topic1']>topics['topic2']) & (topics['topic1']>topics['topic3']), 'lda_topic'] = 1\n",
    "topics.loc[(topics['topic2']>topics['topic1']) & (topics['topic2']>topics['topic3']), 'lda_topic'] = 2\n",
    "topics.loc[(topics['topic3']>topics['topic1']) & (topics['topic3']>topics['topic2']), 'lda_topic'] = 3\n",
    "#topics.loc[(topics['topic4']>topics['topic1']) & (topics['topic4']>topics['topic2']) & (topics['topic4']>topics['topic3']), 'lda_topic'] = 4\n",
    "\n",
    "topics.drop(columns=[0,1,2],inplace = True)\n",
    "\n",
    "# Check results \n",
    "topics.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63fc84",
   "metadata": {},
   "source": [
    "### Topics are relatively evenly distributed across the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79041c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics['lda_topic'].value_counts()/len(topics)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40194ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bind with main dataset\n",
    "df_modeling = pd.concat([df_modeling, topics], axis=1)\n",
    "df_modeling.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749bf68",
   "metadata": {},
   "source": [
    "### More Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling['Primary_Division'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffabeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since MBL does not have many patent applications, combine it into 'Other'\n",
    "df_modeling.loc[df_modeling['Primary_Division']=='MBL','Primary_Division'] = 'Other'\n",
    "\n",
    "df_modeling['Primary_Division'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92977bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create licence counts by primary division\n",
    "num_licenses_division = df_modeling['Primary_Division'].value_counts().reindex(\n",
    "    df_modeling.Primary_Division.unique(), fill_value=0)\n",
    "num_success_licenses_division = df_modeling.loc[df_modeling['License_Status']=='license']['Primary_Division'].value_counts().reindex(\n",
    "    df_modeling.Primary_Division.unique(), fill_value=0)\n",
    "licenses_by_division = pd.DataFrame({'Primary_Division':num_licenses_division.index, 'Licenses_in_Division':num_licenses_division.values, 'Successful Licenses_in_Division':num_success_licenses_division.values})\n",
    "\n",
    "licenses_by_division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create patent success rate by primary division and drop NaN row\n",
    "licenses_by_division['Division_License_Success_Rate'] = licenses_by_division['Successful Licenses_in_Division']/licenses_by_division['Licenses_in_Division']\n",
    "licenses_by_division = licenses_by_division.drop([4])\n",
    "licenses_by_division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling = df_modeling.merge(licenses_by_division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create patent counts by tech family\n",
    "num_licenses_tech = df_modeling['Tech_Title'].value_counts()\n",
    "licenses_by_tech = pd.DataFrame({'Tech_Title':num_licenses_tech.index, 'Licenses_in_Tech':num_licenses_tech.values})\n",
    "licenses_by_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c035e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling = df_modeling.merge(licenses_by_tech)\n",
    "df_modeling.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5546060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time delta between disclosure date and date actually filed \n",
    "df_modeling['Actually_File_Date'] = pd.to_datetime(df_modeling['Actually_File_Date'])\n",
    "df_modeling['Disclosure_Date'] = pd.to_datetime(df_modeling['Disclosure_Date'])\n",
    "\n",
    "df_modeling['Disclosure_to_Filing'] = (df_modeling['Actually_File_Date'] - df_modeling['Disclosure_Date']).astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7cc62",
   "metadata": {},
   "source": [
    "### Imputation of null values not subject to KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b538ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row where topics are null \n",
    "df_modeling = df_modeling.dropna(subset=['topic1'])\n",
    "df_modeling = df_modeling.dropna(subset=['topic2'])\n",
    "df_modeling = df_modeling.dropna(subset=['topic3'])\n",
    "\n",
    "# Convert Owners with null value to \"not listed\"\n",
    "df_modeling.Owners = df_modeling.Owners.fillna('Not_Listed')\n",
    "\n",
    "# Drop Brief_Assessment_Description, Patent_Title, and Tech_Title, as they are description fields and will not add value for our modeling \n",
    "df_modeling.drop('Brief_Technology_Description', axis=1, inplace=True)\n",
    "df_modeling.drop('Tech_Title', axis=1, inplace=True)\n",
    "df_modeling.drop('Patent_Title', axis=1, inplace=True)\n",
    "\n",
    "# Convert Institution with null value to \"Other\"\n",
    "df_modeling.Institution = df_modeling.Institution.fillna('Other')\n",
    "\n",
    "# Market_Size has too many unique values, so we will remove \n",
    "df_modeling.drop('Market_Size', axis=1, inplace=True)\n",
    "\n",
    "# Split out dataset containing records with Disclosure date beyond 2012\n",
    "df_modeling_2012 = df_modeling[(df_modeling['Actually_File_Date']>pd.Timestamp(2012,1,1))]  #last 10-year records\n",
    "\n",
    "# Save Patent Status and remove from dataset for now (we will add back in after scaling)\n",
    "license_status = pd.DataFrame(df_modeling, columns=['License_Status']) \n",
    "df_modeling.drop('License_Status', axis=1, inplace=True)\n",
    "\n",
    "license_status_2012 = pd.DataFrame(df_modeling_2012, columns=['License_Status'])\n",
    "license_status_2012 = license_status_2012.reset_index(drop=True)\n",
    "df_modeling_2012.drop('License_Status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff913",
   "metadata": {},
   "source": [
    "### Create copy of datasets prior to scaling in order to merge actual values with modeling predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of total dataset: ', len(df_modeling))\n",
    "print('Size of 2012 dataset: ', len(df_modeling_2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ddfd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_df_cleaned = df_modeling.copy()\n",
    "license_df_cleaned_2012 = df_modeling_2012.copy()\n",
    "\n",
    "print('Size of total dataset: ', len(license_df_cleaned))\n",
    "print('Size of 2012 dataset: ', len(license_df_cleaned_2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ff19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Tech_ID\n",
    "df_modeling = df_modeling.drop(columns=['Tech_ID'])\n",
    "df_modeling_2012 = df_modeling_2012.drop(columns=['Tech_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abdbcf",
   "metadata": {},
   "source": [
    "### Convert categorical variables using integer encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date fields\n",
    "df_modeling['Disclosure_Date'] = df_modeling['Disclosure_Date'].values.astype(float)\n",
    "df_modeling['Actually_File_Date'] = df_modeling['Actually_File_Date'].values.astype(float)\n",
    "df_modeling['Country_WIPO_ID'] =df_modeling['Country_WIPO_ID'].astype('category').cat.codes\n",
    "df_modeling['Is_Priority'] =df_modeling['Is_Priority'].astype('category').cat.codes\n",
    "df_modeling['Lawfirm'] =df_modeling['Lawfirm'].astype('category').cat.codes\n",
    "df_modeling['Attorney'] =df_modeling['Attorney'].astype('category').cat.codes\n",
    "df_modeling['Application_Type'] =df_modeling['Application_Type'].astype('category').cat.codes\n",
    "df_modeling['Lead_Inventor'] =df_modeling['Lead_Inventor'].astype('category').cat.codes\n",
    "df_modeling['Division_Department'] =df_modeling['Division_Department'].astype('category').cat.codes\n",
    "df_modeling['Primary_Division'] =df_modeling['Primary_Division'].astype('category').cat.codes\n",
    "df_modeling['Owners'] =df_modeling['Owners'].astype('category').cat.codes\n",
    "df_modeling['Ability_of_investigator_to_continue_research'] =df_modeling['Ability_of_investigator_to_continue_research'].astype('category').cat.codes\n",
    "df_modeling['Ability_to_advance_the_project_outside_the_lab'] =df_modeling['Ability_to_advance_the_project_outside_the_lab'].astype('category').cat.codes\n",
    "df_modeling['Compelling_nature_of_data'] =df_modeling['Compelling_nature_of_data'].astype('category').cat.codes\n",
    "df_modeling['Detectability_of_infringement_and_enforceability'] =df_modeling['Detectability_of_infringement_and_enforceability'].astype('category').cat.codes\n",
    "df_modeling['Development_and_regulatory_path_for_the_product'] =df_modeling['Development_and_regulatory_path_for_the_product'].astype('category').cat.codes\n",
    "df_modeling['Freedom-to-operate_FTO_issues'] =df_modeling['Freedom-to-operate_FTO_issues'].astype('category').cat.codes\n",
    "df_modeling['Historical_cooperation_or_not_of_investigator'] =df_modeling['Historical_cooperation_or_not_of_investigator'].astype('category').cat.codes\n",
    "df_modeling['Identity_of_the_eventual_product'] =df_modeling['Identity_of_the_eventual_product'].astype('category').cat.codes\n",
    "df_modeling['Impact_of_patent_on_adoption_of_technology'] =df_modeling['Impact_of_patent_on_adoption_of_technology'].astype('category').cat.codes\n",
    "df_modeling['Industrial_startup_co-ownership_of_the_IP'] =df_modeling['Industrial_startup_co-ownership_of_the_IP'].astype('category').cat.codes\n",
    "df_modeling['Institution'] =df_modeling['Institution'].astype('category').cat.codes\n",
    "df_modeling['Licensing_interest_by_a_specific_company'] =df_modeling['Licensing_interest_by_a_specific_company'].astype('category').cat.codes\n",
    "df_modeling['Market_feedback'] =df_modeling['Market_feedback'].astype('category').cat.codes\n",
    "df_modeling['Nature_of_improvement_over_existing_art'] =df_modeling['Nature_of_improvement_over_existing_art'].astype('category').cat.codes\n",
    "df_modeling['Patentability_questions'] =df_modeling['Patentability_questions'].astype('category').cat.codes\n",
    "df_modeling['Risk_cost_sharing_w_other_institution'] =df_modeling['Risk_cost_sharing_w_other_institution'].astype('category').cat.codes\n",
    "df_modeling['Size_of_Market'] =df_modeling['Size_of_Market'].astype('category').cat.codes\n",
    "df_modeling['Stage_of_research'] =df_modeling['Stage_of_research'].astype('category').cat.codes\n",
    "\n",
    "# date fields\n",
    "df_modeling_2012['Disclosure_Date'] = df_modeling_2012['Disclosure_Date'].values.astype(float)\n",
    "df_modeling_2012['Actually_File_Date'] = df_modeling_2012['Actually_File_Date'].values.astype(float)\n",
    "df_modeling_2012['Country_WIPO_ID'] =df_modeling_2012['Country_WIPO_ID'].astype('category').cat.codes\n",
    "df_modeling_2012['Is_Priority'] =df_modeling_2012['Is_Priority'].astype('category').cat.codes\n",
    "df_modeling_2012['Lawfirm'] =df_modeling_2012['Lawfirm'].astype('category').cat.codes\n",
    "df_modeling_2012['Attorney'] =df_modeling_2012['Attorney'].astype('category').cat.codes\n",
    "df_modeling_2012['Application_Type'] =df_modeling_2012['Application_Type'].astype('category').cat.codes\n",
    "df_modeling_2012['Lead_Inventor'] =df_modeling_2012['Lead_Inventor'].astype('category').cat.codes\n",
    "df_modeling_2012['Division_Department'] =df_modeling_2012['Division_Department'].astype('category').cat.codes\n",
    "df_modeling_2012['Primary_Division'] =df_modeling_2012['Primary_Division'].astype('category').cat.codes\n",
    "df_modeling_2012['Owners'] =df_modeling_2012['Owners'].astype('category').cat.codes\n",
    "df_modeling_2012['Ability_of_investigator_to_continue_research'] =df_modeling_2012['Ability_of_investigator_to_continue_research'].astype('category').cat.codes\n",
    "df_modeling_2012['Ability_to_advance_the_project_outside_the_lab'] =df_modeling_2012['Ability_to_advance_the_project_outside_the_lab'].astype('category').cat.codes\n",
    "df_modeling_2012['Compelling_nature_of_data'] =df_modeling_2012['Compelling_nature_of_data'].astype('category').cat.codes\n",
    "df_modeling_2012['Detectability_of_infringement_and_enforceability'] =df_modeling_2012['Detectability_of_infringement_and_enforceability'].astype('category').cat.codes\n",
    "df_modeling_2012['Development_and_regulatory_path_for_the_product'] =df_modeling_2012['Development_and_regulatory_path_for_the_product'].astype('category').cat.codes\n",
    "df_modeling_2012['Freedom-to-operate_FTO_issues'] =df_modeling_2012['Freedom-to-operate_FTO_issues'].astype('category').cat.codes\n",
    "df_modeling_2012['Historical_cooperation_or_not_of_investigator'] =df_modeling_2012['Historical_cooperation_or_not_of_investigator'].astype('category').cat.codes\n",
    "df_modeling_2012['Identity_of_the_eventual_product'] =df_modeling_2012['Identity_of_the_eventual_product'].astype('category').cat.codes\n",
    "df_modeling_2012['Impact_of_patent_on_adoption_of_technology'] =df_modeling_2012['Impact_of_patent_on_adoption_of_technology'].astype('category').cat.codes\n",
    "df_modeling_2012['Industrial_startup_co-ownership_of_the_IP'] =df_modeling_2012['Industrial_startup_co-ownership_of_the_IP'].astype('category').cat.codes\n",
    "df_modeling_2012['Institution'] =df_modeling_2012['Institution'].astype('category').cat.codes\n",
    "df_modeling_2012['Licensing_interest_by_a_specific_company'] =df_modeling_2012['Licensing_interest_by_a_specific_company'].astype('category').cat.codes\n",
    "df_modeling_2012['Market_feedback'] =df_modeling_2012['Market_feedback'].astype('category').cat.codes\n",
    "df_modeling_2012['Nature_of_improvement_over_existing_art'] =df_modeling_2012['Nature_of_improvement_over_existing_art'].astype('category').cat.codes\n",
    "df_modeling_2012['Patentability_questions'] =df_modeling_2012['Patentability_questions'].astype('category').cat.codes\n",
    "df_modeling_2012['Risk_cost_sharing_w_other_institution'] =df_modeling_2012['Risk_cost_sharing_w_other_institution'].astype('category').cat.codes\n",
    "df_modeling_2012['Size_of_Market'] =df_modeling_2012['Size_of_Market'].astype('category').cat.codes\n",
    "df_modeling_2012['Stage_of_research'] =df_modeling_2012['Stage_of_research'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c738ddc",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7e730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_modeling = pd.DataFrame(scaler.fit_transform(df_modeling), columns = df_modeling.columns)\n",
    "df_modeling_2012 = pd.DataFrame(scaler.fit_transform(df_modeling_2012), columns = df_modeling_2012.columns)\n",
    "df_modeling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c68e997",
   "metadata": {},
   "source": [
    "### Conduct KNN Imputation\n",
    "\n",
    "Resources: https://medium.com/@kyawsawhtoon/a-guide-to-knn-imputation-95e2dc496e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38542954",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_modeling = pd.DataFrame(imputer.fit_transform(df_modeling),columns = df_modeling.columns)\n",
    "df_modeling_2012 = pd.DataFrame(imputer.fit_transform(df_modeling_2012),columns = df_modeling_2012.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364839a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modeling.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge patent status back onto modeling dataset \n",
    "df_modeling = df_modeling.join(license_status)\n",
    "df_modeling_2012 = df_modeling_2012.join(license_status_2012)\n",
    "df_modeling_2012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23f95c",
   "metadata": {},
   "source": [
    "### 6. Review Final Patent Status Breakout and length of Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26712e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_modeling_2012['License_Status'].value_counts()/len(df_modeling_2012)*100)\n",
    "\n",
    "print('\\nSize of total dataset: ', len(license_df_cleaned))\n",
    "print('Size of 2012 dataset: ', len(license_df_cleaned_2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef598cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_modeling_2012.copy()\n",
    "\n",
    "# drop unamed column \n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# Remove row with null patent_status\n",
    "df = df.dropna(subset=['License_Status'])\n",
    "\n",
    "# convert funding status to binary \n",
    "df.loc[df['License_Status']=='no_license', 'License_Status'] = 0\n",
    "df.loc[df['License_Status']=='license', 'License_Status'] = 1\n",
    "df[\"License_Status\"] = df.License_Status.astype(float)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:38]\n",
    "y = df.iloc[:, 38]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "\n",
    "# check split of data\n",
    "len(x_train), len(y_train), len(x_test), len(y_test), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employment of SMOTE to oversample skewed funding outputs. This yields better results \n",
    "sm = SMOTE()\n",
    "x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "# check shape of x_train and y_train, and new response variable ratio\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171221d9",
   "metadata": {},
   "source": [
    "### 6.1 Binary Classification Modeling Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ae617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "# Standard binary classification models \n",
    "pipelines.append(('LogisticRegression', Pipeline([('LR',linear_model.LogisticRegression())])))\n",
    "pipelines.append(('KNearestNeighbors', Pipeline([('KNN',KNeighborsClassifier())])))\n",
    "pipelines.append(('LinearSVC', Pipeline([('SVC',LinearSVC())])))\n",
    "pipelines.append(('DecisionTree', Pipeline([('DTREE',DecisionTreeClassifier())])))\n",
    "# Employment of ensemble learning \n",
    "pipelines.append(('BaggingClassifier', Pipeline([('BAG',BaggingClassifier())])))\n",
    "pipelines.append(('BoostClassifier', Pipeline([('BOOST',AdaBoostClassifier())])))\n",
    "pipelines.append(('RandomForest', Pipeline([('FOREST',RandomForestClassifier())])))\n",
    "pipelines.append(('GradientBoost', Pipeline([('GBoost',GradientBoostingClassifier())])))\n",
    "pipelines.append(('XGBoosting', Pipeline([('XGBoost',XGBClassifier(objective='binary:logistic', eval_metric='error'))])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='f1')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84485c",
   "metadata": {},
   "source": [
    "### 6.2 Model Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve outputs \n",
    "def roc_curve(model):\n",
    "    probs = model.predict_proba(x_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='GridSearchCV (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('Log_ROC')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Model fitting outputs \n",
    "def fitting_score(train_predictions, test_predictions, model):\n",
    "    # Evaluate the train, test and CV metrics\n",
    "    # train\n",
    "    train_recall = metrics.recall_score(y_train, train_predictions, average='macro')\n",
    "    train_precision = metrics.precision_score(y_train, train_predictions, average='macro')\n",
    "    train_f1 = metrics.f1_score(y_train, train_predictions, average='macro')\n",
    "\n",
    "    # test\n",
    "    test_recall = metrics.recall_score(y_test, test_predictions, average='macro')\n",
    "    test_precision = metrics.precision_score(y_test, test_predictions, average='macro')\n",
    "    test_f1 = metrics.f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "    # CV\n",
    "    cv_recall = cross_val_score(model, x_train, y_train, cv=5, scoring='recall', n_jobs=-1).mean()\n",
    "    cv_precision = cross_val_score(model, x_train, y_train, cv=5, scoring='precision', n_jobs=-1).mean()\n",
    "    cv_f1 = cross_val_score(model, x_train, y_train, cv=5, scoring='f1', n_jobs=-1).mean()\n",
    "    \n",
    "    # Put everything in a table\n",
    "    d = {'train': [train_recall, train_precision, train_f1], 'test': [test_recall, test_precision, test_f1],\n",
    "        'cv':[cv_recall, cv_precision, cv_f1]}\n",
    "    score_df = pd.DataFrame(data=d, index=['recall', 'precision', 'f1'])\n",
    "    return score_df.round(decimals=3)\n",
    "\n",
    "def pr_graph(model):\n",
    "    # predict probabilities\n",
    "    lr_probs = model.predict_proba(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # predict class values\n",
    "    yhat = model.predict(x_test)\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "    lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('Recall')\n",
    "    pyplot.ylabel('Precision')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=42, objective='binary:logistic', eval_metric='error', gamma=5,\n",
    "                   colsample_bytree=0.35, learning_rate=0.1, n_estimators=200, reg_lambda=1, max_depth=15,\n",
    "                   min_child_weight=0, reg_alpha=1, scale_pos_weight=2, subsample=0.9)\n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "xgb_predictions = xgb.predict(x_test)\n",
    "xgb_predictions_train = xgb.predict(x_train)\n",
    "\n",
    "xgb_matrix = metrics.confusion_matrix(y_test,xgb_predictions)\n",
    "\n",
    "plot_confusion_matrix(xgb, x_test, y_test)  \n",
    "plt.show()\n",
    "\n",
    "class_report_xgb = classification_report(y_test, xgb_predictions)\n",
    "print(\"\\nGradient Boosting Classifier Confusion Matrix\\n\",class_report_xgb)\n",
    "\n",
    "print(\"\\nAccuracy:\",round(metrics.accuracy_score(y_test,xgb_predictions),3))\n",
    "\n",
    "print('\\n Gradient Boosting Classification Train/Test/CV Scoring')\n",
    "fitting_score(xgb_predictions_train, xgb_predictions, xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa19452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9744d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c755219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee30e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
